{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Nerual network architecture will be as follows..\n",
    "--Input will have three variables with 4 Feaures\n",
    "--Will have a our filter which will have same no of channel as Input and hence our Filter will be (4,3)\n",
    "--Three Bias values will be available as we have three input\n",
    "--Have one hidden layer\n",
    "--Activation Function is being assumed as Sigmoid\n",
    "--Outputs have been clarified and Response will be compared with Output to calculate the Error\n",
    "--Weights for Kernel and Hidden layer will be updated based on error to minimise the error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section explains the Input value which is is matrix. We need to set the matrix element (1,1), (1,2), (2.1),(2,3), (2,4), (3,2) and (3,4) to 1. This input will then be passed through our kernel.\n",
    "Note: Here we have three Input and their 4 feature values. So the matrix size is (3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back Propagation has following phases and they are\n",
    "-Initialisation of weights for Kernel and Hidden filters\n",
    "    -Initialise all the weights for all the neurals to some dandom values    \n",
    "-Feed Forward from Input layer to output of layers\n",
    "    -Propagate input to hidden and output layers\n",
    "    -Calculate input to hidden layer and output of hidden layers\n",
    "    -Calculate input to the Output layer\n",
    "-Backprogataion of Errors\n",
    "    -Calculate error for output layers neurons\n",
    "-Updation of weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import plotly as pt\n",
    "import matplotlib as mp\n",
    "import sys as sy\n",
    "import random as rd\n",
    "from scipy.stats import logistic as lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Input Is [[1. 0. 1. 0.]\n",
      " [1. 0. 1. 1.]\n",
      " [0. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "row =3\n",
    "column =4\n",
    "InputValue = np.zeros(shape=(row,column))\n",
    "Input_matrix = np.matrix(np.array(InputValue))\n",
    "\n",
    "print(Input_matrix)\n",
    "Input_matrix[0,0] =1\n",
    "Input_matrix[0,2] =1\n",
    "Input_matrix[1,0] =1\n",
    "Input_matrix[1,2] =1\n",
    "Input_matrix[1,3] =1\n",
    "Input_matrix[2,1] =1\n",
    "Input_matrix[2,3] =1\n",
    "print(\"Input Is\",Input_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section explains our kernel with random values as weightage. Initialize weights and biases with random values. Note: Here the Bias has three value which is alligned with 3 Input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.60566074 0.48779029 0.26935985]\n",
      " [0.47519452 0.83643715 0.12231566]\n",
      " [0.00832356 0.02858857 0.30642845]\n",
      " [0.84187443 0.70294747 0.83017049]]\n",
      "Filter Weights: [[0.60566074 0.48779029 0.26935985]\n",
      " [0.47519452 0.83643715 0.12231566]\n",
      " [0.00832356 0.02858857 0.30642845]\n",
      " [0.84187443 0.70294747 0.83017049]]\n",
      "Bias Values: [[0.90037082 0.98057824 0.66305548]]\n"
     ]
    }
   ],
   "source": [
    "## Defining the kernel weights\n",
    "wh = np.random.rand(4,3)\n",
    "print(wh)\n",
    "wh_matrix = np.matrix(np.array(wh))\n",
    "print(\"Filter Weights:\", wh_matrix)\n",
    "\n",
    "## Now defining the bias values\n",
    "bias = np.random.rand(1,3)\n",
    "bias_matrix = np.matrix(np.array(bias))\n",
    "print(\"Bias Values:\", bias_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following section calculate input of Hidden layer. We have Input matrix, then Kernel and so Output from kernel will be dot product of Input and Kernel matrix. Then bias will be added with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Kernel: [[0.61398429 0.51637885 0.5757883 ]\n",
      " [1.45585872 1.21932632 1.40595878]\n",
      " [1.31706895 1.53938462 0.95248615]]\n",
      "Output of Kernel after Bias Addition: [[1.51435512 1.49695709 1.23884378]\n",
      " [2.35622955 2.19990457 2.06901427]\n",
      " [2.21743977 2.51996286 1.61554163]]\n"
     ]
    }
   ],
   "source": [
    "## here we are doing dot product of two matrix\n",
    "Output_Kernel = np.dot(Input_matrix,wh_matrix)\n",
    "print(\"Output of Kernel:\", Output_Kernel)\n",
    "Output_Kernel_Bias = np.add(Output_Kernel,bias_matrix)\n",
    "print(\"Output of Kernel after Bias Addition:\", Output_Kernel_Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function is meant for sigmoid derivative\n",
    "def sigmoid_mod(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our activation function is taken as sigmoid one. So all input to our hidden network will be going through sigmoid activation function to have output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Hidden Layer: [[0.81970574 0.8171202  0.77536269]\n",
      " [0.91342811 0.90024094 0.88785485]\n",
      " [0.90180471 0.9255295  0.83417935]]\n"
     ]
    }
   ],
   "source": [
    "Input_Hidden_layer = Output_Kernel_Bias\n",
    "##Output_hidden_layer = lg.cdf(Input_Hidden_layer)\n",
    "Output_hidden_layer = sigmoid_mod(Input_Hidden_layer)  ### applying sigmoid\n",
    "\n",
    "\n",
    "print(\"Output of Hidden Layer:\", Output_hidden_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section first defines the weights for hindden layer and then bias fo hidden layer. Hiddden layer has activation function and output from activation function is a matrix is (3,3). So our weights for hidden layer will have only 3 variables with one feature. We will take these weights as random ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Weights: [[0.90703588]\n",
      " [0.84843077]\n",
      " [0.29526194]]\n",
      "Hidden Layer Bias Value: [[0.89919192]]\n"
     ]
    }
   ],
   "source": [
    "###Defining the weights for hidden layer\n",
    "wout = np.random.rand(3,1)\n",
    "wout_matrix = np.matrix(np.array(wout))\n",
    "print(\"Hidden Layer Weights:\", wout_matrix)\n",
    "##defining the bias for hidden layer\n",
    "wout_bias = np.random.rand(1,1)\n",
    "print(\"Hidden Layer Bias Value:\", wout_bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section explains how the hidden layer neural network applies weights and then gets influenced by bias at the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer output after weight: [[1.66570753]\n",
      " [1.85445393]\n",
      " [1.84951834]]\n",
      "Hidden layer output after Bias: [[2.56489944]\n",
      " [2.75364585]\n",
      " [2.74871026]]\n"
     ]
    }
   ],
   "source": [
    "Output_hidden_layer_after_weight = np.dot(Output_hidden_layer,wout_matrix)\n",
    "print(\"Hidden layer output after weight:\", Output_hidden_layer_after_weight)\n",
    "\n",
    "Output_hidden_layer_after_bias = np.add(Output_hidden_layer_after_weight,wout_bias)\n",
    "print(\"Hidden layer output after Bias:\", Output_hidden_layer_after_bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our expected output is a having three input value as 1,1,0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Output: [[1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "rowOut =3\n",
    "columnOut =1\n",
    "OutputValue = np.zeros(shape=(rowOut,columnOut))\n",
    "OutputValue_matrix = np.matrix(np.array(OutputValue))\n",
    "OutputValue_matrix[0,0] = 1\n",
    "OutputValue_matrix[1,0] = 1\n",
    "print(\"Expected Output:\", OutputValue_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now calculate the error. Error will be Final output - Expected Output. Back Propagation is the way the weights are trained into each of the layers and kernel. Actually these errors are then passed back from putput to hidden layer and then to kernel as similar to passing the blame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.56489944]\n",
      " [2.75364585]\n",
      " [2.74871026]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Error Term for Hidden Layer: [[-1.56489944]\n",
      " [-1.75364585]\n",
      " [-2.74871026]]\n"
     ]
    }
   ],
   "source": [
    "print(Output_hidden_layer_after_bias)\n",
    "print(OutputValue_matrix)\n",
    "Gradient_Error_Value_1st_Iter = np.subtract(OutputValue_matrix,Output_hidden_layer_after_bias)\n",
    "print(\"Error Term for Hidden Layer:\", Gradient_Error_Value_1st_Iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is splitted into two section and they are -Transfer Derivative and -Error Backpropagation. The hidden layer has sigmoid activation function and hence and the derivative of the same will be as follows. Aim is to adjust each weight in the network in proportion to how much it contributes to overall error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function is meant for sigmoid derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.55230791]\n",
      " [0.549834  ]\n",
      " [0.31216867]]\n"
     ]
    }
   ],
   "source": [
    "k = sigmoid(Gradient_Error_Value_1st_Iter)\n",
    "#print(k)\n",
    "\n",
    "kky = np.random.rand(3,1)\n",
    "\n",
    "wout_kky = np.matrix(np.array(kky))\n",
    "wout_kky[0,0] = 0.21\n",
    "wout_kky[1,0] = 0.20\n",
    "wout_kky[2,0] = -0.79\n",
    "mk = sigmoid(wout_kky)\n",
    "print(mk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now calculate the Output slope. Slope for sigmoid activation function will be calculated as value(1-value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Slope: [[4.01]\n",
      " [4.83]\n",
      " [4.81]]\n",
      "Output_hidden_layer_after_bias: [[2.56489944]\n",
      " [2.75364585]\n",
      " [2.74871026]]\n",
      "Output_slope_delta: [[1.56489944]\n",
      " [1.75364585]\n",
      " [1.74871026]]\n",
      "Output_slope_matrix: [[4.01]\n",
      " [4.83]\n",
      " [4.81]]\n",
      "Gradient_Error_Value_1st_Iter: [[-1.56489944]\n",
      " [-1.75364585]\n",
      " [-2.74871026]]\n",
      "delta_output_m [[ -6.28]\n",
      " [ -8.47]\n",
      " [-13.22]]\n"
     ]
    }
   ],
   "source": [
    "Output_slope_delta = np.subtract(Output_hidden_layer_after_bias,1)\n",
    "#Output_slope = np.dot(Output_slope_delta,Output_hidden_layer_after_bias) ## output slope calculated here\n",
    "rowOut =3\n",
    "columnOut =1\n",
    "Output_slope = np.zeros(shape=(rowOut,columnOut))\n",
    "Output_slope_matrix = np.matrix(np.array(Output_slope))\n",
    "\n",
    "#m = value * value1\n",
    "#v = round(m,2)\n",
    "#Output_slope_matrix[0,0] = v\n",
    "num = 0\n",
    "for num in range(0,3):     #to iterate between 10 to 20\n",
    "    Output_slope_matrix[num,0] = round((Output_hidden_layer_after_bias[num,0] * Output_slope_delta[num,0]) ,2)\n",
    "\n",
    "    \n",
    "print(\"Output Slope:\",Output_slope_matrix)\n",
    "print(\"Output_hidden_layer_after_bias:\",Output_hidden_layer_after_bias)      \n",
    "print(\"Output_slope_delta:\",Output_slope_delta) \n",
    "### Now we need to multiply the slope and delta output. Delta output will be driven by error which is the \n",
    "rowOut =3\n",
    "columnOut =1\n",
    "delta_output = np.zeros(shape=(rowOut,columnOut))\n",
    "delta_output_m = np.matrix(np.array(Output_slope))\n",
    "\n",
    "for num in range(0,3):     #to iterate between 10 to 20\n",
    "    delta_output_m[num,0] = round((Output_slope_matrix[num,0] * Gradient_Error_Value_1st_Iter[num,0]) ,2)\n",
    "    \n",
    "print(\"Output_slope_matrix:\",Output_slope_matrix) \n",
    "print(\"Gradient_Error_Value_1st_Iter:\",Gradient_Error_Value_1st_Iter) \n",
    "print(\"delta_output_m\",delta_output_m) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have calculated delta output and next task is to calculate error at hidden layer, slope at hidden layer and then delta at hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error_Hidden_layer [[ -5.69618531  -5.32814521  -1.854245  ]\n",
      " [ -7.68259388  -7.18620859  -2.50086866]\n",
      " [-11.9910143  -11.21625473  -3.90336289]]\n",
      "Slope_hidden_layer [[1.66570753]\n",
      " [1.85445393]\n",
      " [1.84951834]]\n",
      "derivative_hiddenlayer [[-17.49298524 -16.36273409  -5.69438645]\n",
      " [-18.02269226 -16.85821585  -5.8668188 ]\n",
      " [-17.91442511 -16.75694402  -5.83157524]]\n"
     ]
    }
   ],
   "source": [
    "### First step is to calculate error at hidden layer\n",
    "wout_matrix_T = np.transpose(wout_matrix)\n",
    "Error_Hidden_layer = np.dot(delta_output_m,wout_matrix_T)\n",
    "print(\"Error_Hidden_layer\",Error_Hidden_layer) \n",
    "### Note here have taken the output of the hidden layer and this output is before applying the bias and weights. So this is again as part\n",
    "## of back progation we are doing sigmoid\n",
    "Slope_hidden_layer = sigmoid(Output_hidden_layer)\n",
    "\n",
    "print(\"Slope_hidden_layer\",Output_hidden_layer_after_weight) \n",
    "## we will now calculate the derivative which is multipliction of slope and error\n",
    "derivative_hiddenlayer = Slope_hidden_layer * Error_Hidden_layer\n",
    "print(\"derivative_hiddenlayer\",derivative_hiddenlayer) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute delta at hidden layer and learning_rate is asssumed as 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wout_matrix_bp [[-8.66231487 -8.04402305 -2.20801741]\n",
      " [-8.72091998 -8.10262816 -2.26662253]\n",
      " [-9.2740888  -8.65579698 -2.81979135]]\n",
      "wout_matrix_bp [[-2.94590701 -2.83430471 -0.88676067]\n",
      " [-1.31624799 -0.83925726 -0.46084186]\n",
      " [-3.54324419 -3.29350643 -0.84969208]\n",
      " [-2.75183731 -2.65856852 -0.33966892]]\n"
     ]
    }
   ],
   "source": [
    "learning_rate =0.1\n",
    "Output_hidden_layer_after_weight_T = np.transpose(Output_hidden_layer_after_weight)\n",
    "## we are calculating the hidden layer weights now with back propagation\n",
    "wout_matrix_bp = wout_matrix + np.dot(Output_hidden_layer_after_weight_T,derivative_hiddenlayer) * learning_rate\n",
    "print(\"wout_matrix_bp\",wout_matrix_bp) \n",
    "# we are now calculating the initial kernel filter weights\n",
    "Input_matrix_T = np.transpose(Input_matrix)\n",
    "wh_matrix_bp = wh_matrix + np.dot(Input_matrix_T,derivative_hiddenlayer) * learning_rate\n",
    "print(\"wout_matrix_bp\",wh_matrix_bp) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section explains our intial randomly assigned weights and newly calculated weights after backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights of Filter: [[0.60566074 0.48779029 0.26935985]\n",
      " [0.47519452 0.83643715 0.12231566]\n",
      " [0.00832356 0.02858857 0.30642845]\n",
      " [0.84187443 0.70294747 0.83017049]]\n",
      "Revised Weights of Filter After Back Propagation: [[-8.66231487 -8.04402305 -2.20801741]\n",
      " [-8.72091998 -8.10262816 -2.26662253]\n",
      " [-9.2740888  -8.65579698 -2.81979135]]\n",
      "Initial Weights of Hidden Layer: [[0.60566074 0.48779029 0.26935985]\n",
      " [0.47519452 0.83643715 0.12231566]\n",
      " [0.00832356 0.02858857 0.30642845]\n",
      " [0.84187443 0.70294747 0.83017049]]\n",
      "Revised Weights of Hidden Layer After Back Propagation: [[-2.94590701 -2.83430471 -0.88676067]\n",
      " [-1.31624799 -0.83925726 -0.46084186]\n",
      " [-3.54324419 -3.29350643 -0.84969208]\n",
      " [-2.75183731 -2.65856852 -0.33966892]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Weights of Filter:\", wh_matrix)\n",
    "print(\"Revised Weights of Filter After Back Propagation:\", wout_matrix_bp)\n",
    "\n",
    "print(\"Initial Weights of Hidden Layer:\", wh_matrix)\n",
    "print(\"Revised Weights of Hidden Layer After Back Propagation:\", wh_matrix_bp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do the backpropagation excercise with Hidden layer having activation function of ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output_hidden_layer_reLU [[0.81970574 0.8171202  0.77536269]\n",
      " [0.91342811 0.90024094 0.88785485]\n",
      " [0.90180471 0.9255295  0.83417935]]\n",
      "Hidden layer output after weight-reLU: [[1.66570753]\n",
      " [1.85445393]\n",
      " [1.84951834]]\n",
      "Hidden layer output after Bias-reLU: [[2.56489944]\n",
      " [2.75364585]\n",
      " [2.74871026]]\n",
      "[[2.56489944]\n",
      " [2.75364585]\n",
      " [2.74871026]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Error Term for Hidden Layer-reLU: [[-1.56489944]\n",
      " [-1.75364585]\n",
      " [-2.74871026]]\n",
      "Error_Hidden_layer-reLU [[ -5.69618531  -5.32814521  -1.854245  ]\n",
      " [ -7.68259388  -7.18620859  -2.50086866]\n",
      " [-11.9910143  -11.21625473  -3.90336289]]\n",
      "Slope_hidden_layer [[1.66570753]\n",
      " [1.85445393]\n",
      " [1.84951834]]\n",
      "derivative_hiddenlayer [[-20.24418357 -18.93617288  -6.58996753]\n",
      " [-22.76552155 -21.29460298  -7.41072354]\n",
      " [-22.24997048 -20.81236253  -7.2428993 ]]\n",
      "wout_matrix_bp [[-10.80198696 -10.04544733  -2.90453202]\n",
      " [-10.86059207 -10.10405244  -2.96313713]\n",
      " [-11.41376089 -10.65722126  -3.51630596]]\n",
      "wh_matrix_bp_reLU [[-3.69530978 -3.5352873  -1.13070926]\n",
      " [-1.74980253 -1.24479911 -0.60197427]\n",
      " [-4.29264695 -3.99448902 -1.09364066]\n",
      " [-3.65967478 -3.50774908 -0.6351918 ]]\n"
     ]
    }
   ],
   "source": [
    "## As all the values are +ve so the reLU will just pass on the values.\n",
    "Output_hidden_layer_reLU = Output_hidden_layer\n",
    "print(\"Output_hidden_layer_reLU\", Output_hidden_layer_reLU)\n",
    "\n",
    "Output_hidden_layer_after_weight_reLU = np.dot(Output_hidden_layer_reLU,wout_matrix)\n",
    "print(\"Hidden layer output after weight-reLU:\", Output_hidden_layer_after_weight)\n",
    "\n",
    "Output_hidden_layer_after_bias_reLU = np.add(Output_hidden_layer_after_weight_reLU,wout_bias)\n",
    "print(\"Hidden layer output after Bias-reLU:\", Output_hidden_layer_after_bias)\n",
    "### calculating error values\n",
    "print(Output_hidden_layer_after_bias_reLU)\n",
    "print(OutputValue_matrix)\n",
    "Gradient_Error_Value_1st_Iter_reLU = np.subtract(OutputValue_matrix,Output_hidden_layer_after_bias_reLU)\n",
    "print(\"Error Term for Hidden Layer-reLU:\", Gradient_Error_Value_1st_Iter_reLU)\n",
    "#####Now calculating the gradient for back propagation\n",
    "learning_rate =0.1\n",
    "Output_hidden_layer_after_weight_T_reLU = np.transpose(Output_hidden_layer_after_weight_reLU)\n",
    "\n",
    "## This function is meant for reLU derivative\n",
    "def reLU_mod(x):\n",
    "    if  x > 0:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "#####\n",
    "### First step is to calculate error at hidden layer\n",
    "wout_matrix_T_reLU = np.transpose(wout_matrix)\n",
    "Error_Hidden_layer_reLU = np.dot(delta_output_m,wout_matrix_T_reLU)\n",
    "print(\"Error_Hidden_layer-reLU\",Error_Hidden_layer_reLU) \n",
    "### Note here have taken the output of the hidden layer and this output is before applying the bias and weights. So this is again as part\n",
    "## of back progation we are doing sigmoid\n",
    "Slope_hidden_layer_reLU = Output_hidden_layer_reLU ## here this slope is 1 as all numbers are negative  ##as a #reLU_mod(Output_hidden_layer_reLU)\n",
    "\n",
    "print(\"Slope_hidden_layer\",Output_hidden_layer_after_weight) \n",
    "## we will now calculate the derivative which is multipliction of slope and error\n",
    "derivative_hiddenlayer_reLU = Slope_hidden_layer_reLU * Error_Hidden_layer\n",
    "print(\"derivative_hiddenlayer\",derivative_hiddenlayer) \n",
    "\n",
    "## we are calculating the hidden layer weights now with back propagation\n",
    "wout_matrix_bp_reLU = wout_matrix + np.dot(Output_hidden_layer_after_weight_T_reLU,derivative_hiddenlayer) * learning_rate\n",
    "\n",
    "print(\"wout_matrix_bp\",wout_matrix_bp_reLU) \n",
    "# we are now calculating the initial kernel filter weights\n",
    "Input_matrix_T = np.transpose(Input_matrix)\n",
    "wh_matrix_bp_reLU = wh_matrix + np.dot(Input_matrix_T,derivative_hiddenlayer_reLU) * learning_rate\n",
    "print(\"wh_matrix_bp_reLU\",wh_matrix_bp_reLU) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
